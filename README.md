# LLM Parameter Calculation

This repository contains documentation and code for calculating LLM parameters, memory usage, and tokenization.

## Folders

### Parameter_Calculations/
Memory and performance calculation guides for various LLM models:
- **Activation memory calculations** - Understanding activation memory requirements during training
- **GPT-3 memory and FLOPS calculations** - Detailed analysis of memory and floating-point operations for GPT-3
- **Llama2 and Llama3 memory analysis** - Memory calculations for Meta's Llama model series
- **LLM finetuning memory requirements** - Memory considerations for finetuning large language models
- **LLM inference optimization** - Optimization strategies for LLM inference
- **Model size calculations** - How to calculate the size of LLM models
- **Training and inference FLOPS calculations** - Floating-point operations analysis
- **ZeRO optimization memory considerations** - Memory implications of ZeRO (Zero Redundancy Optimizer)

### Tokenizer/
Tokenizer implementations for LLM processing:
- **Llama3 tokenizer** - Implementation of the Llama3 tokenizer

## Usage
The markdown files and PDFs in the Parameter_Calculations folder provide formulas, examples, and detailed explanations for calculating various aspects of LLM memory usage and performance. The Tokenizer folder contains working code for tokenizing text using the Llama3 tokenizer.
